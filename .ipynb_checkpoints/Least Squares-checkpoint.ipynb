{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema de Otimização Não-Linear\n",
    "## Projeto da disciplina **SME5720 - Otimização Não-linear**\n",
    "### Estudo de caso da Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membros\n",
    "\n",
    "* Eduardo Zaffari Monteiro - eduardozaffarimonteiro@usp.br - 12559490\n",
    "\n",
    "* Gustavo Silva de Oliveira - gustavo.oliveira03@usp.br - 12567231\n",
    "\n",
    "* Lucas Ivars Cadima Ciziks - luciziks@usp.br - 125599472\n",
    "\n",
    "* Pedro Henrique de Freitas Maçonetto - pedromaconetto@usp.br - 12675419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicar qual o problema escolhido e os métodos que foram implementados, bem como que tipo de problemas eles resolvem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descrever a modelagem matemática do problema e deverá ser apontado que tipo de método pode ser usado para resolvê-lo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import pandas as pd\n",
    "\n",
    "def find_step_length(f, fd, x, alpha, direction, c2):\n",
    "    g = lambda alpha: f(x+alpha*direction)\n",
    "    gd = lambda alpha: np.dot(fd(x + alpha*direction), direction)\n",
    "    return interpolation(g, gd, alpha, c2)\n",
    "\n",
    "def wolf1(f, fd, alpha):\n",
    "    c1 = 1e-4\n",
    "    return f(alpha) <= f(0) + c1*alpha*fd(alpha)\n",
    "\n",
    "def wolf_strong(f, fd, alpha, c2):\n",
    "    return abs(fd(alpha)) <= -c2*fd(0)\n",
    "\n",
    "def simple_backtracking(f, fd, alpha, c2):\n",
    "    rate = 0.5\n",
    "    while not (wolf1(f, fd, alpha) or wolf_strong(f, fd, alpha, c2)):\n",
    "        alpha = rate*alpha\n",
    "    return alpha\n",
    "\n",
    "def interpolation(f, fd, alpha, c2):\n",
    "    lo = 0.0\n",
    "    hi = 1.0\n",
    "    \n",
    "    for i in range(0, 20):\n",
    "        if wolf1(f, fd, alpha):\n",
    "            if wolf_strong(f, fd, alpha, c2):\n",
    "                return alpha\n",
    "    \n",
    "    half = (lo+hi)/2.0\n",
    "    alpha = - (fd(lo)*hi*hi) / (2*(f(hi)-f(lo)-fd(lo)*hi))\n",
    "    \n",
    "    if alpha < lo or alpha > hi: # quadratic interpolation failed. reduce by half instead\n",
    "        alpha = half\n",
    "    if fd(alpha) > 0:\n",
    "        hi = alpha\n",
    "    elif fd(alpha) <= 0:\n",
    "        lo = alpha\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f, fd, x, max_iterations, precision, callback):\n",
    "    direction = -fd(x)\n",
    "    gradient = None\n",
    "    gradient_next = matrix(fd(x)).T\n",
    "    x_prev = None\n",
    "  \n",
    "    for i in range(1, max_iterations):\n",
    "        alpha = find_step_length(f, fd, x, 1.0, direction, c2=0.1)\n",
    "        x_prev = x\n",
    "        x = x + alpha*direction\n",
    "\n",
    "        callback(i, direction, alpha, x)\n",
    "\n",
    "        gradient = gradient_next\n",
    "        gradient_next = matrix(fd(x)).T\n",
    "\n",
    "        if linalg.norm(x - x_prev) < precision:\n",
    "            break\n",
    "    \n",
    "        BFR = (gradient_next.T * gradient_next) / (gradient.T * gradient)\n",
    "        BFR = squeeze(asarray(BFR))\n",
    "    \n",
    "        direction = -squeeze(asarray(gradient_next)) + BFR*direction\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return x[0]**2 + x[1]**2\n",
    "def df_dx1(x): return 2*x[0]\n",
    "def df_dx2(x): return 2*x[1]\n",
    "def fd(x): return array([ df_dx1(x), df_dx2(x) ])\n",
    "\n",
    "def print_error(i, direction, alpha, x):\n",
    "    opt = f(array([1,1]))\n",
    "    print(\"%d, %.20f\" % (i, f(x)-opt))\n",
    "  \n",
    "def print_gradient(i, direction, alpha, x):\n",
    "    print(\"%d, %.20f\" % (i, linalg.norm(fd(x))))\n",
    "  \n",
    "def print_all(i, direction, alpha, x):\n",
    "    print(\"iteration %d: \\t direction: %s \\t alpha: %.7f \\t x: %s\"% (i, [\"%.7f\" % _ for _ in direction], alpha, [\"%.7f\" % _ for _ in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return ((2.40 - x[0] - x[1]*68)**2 + (4.70 - x[0] - x[1]*137)**2 + (12 - x[0] - x[1]*315)**2 + \n",
    "(14.44 - x[0] - x[1]*405)**2 + (26 - x[0] - x[1]*700)**2)\n",
    "\n",
    "def df_dx1(x): return (-2) * ((2.40 - x[0] - x[1]*68) + (4.70 - x[0] - x[1]*137) + (12 - x[0] - x[1]*315) + \n",
    "(14.44 - x[0] - x[1]*405) + (26 - x[0] - x[1]*700))\n",
    "\n",
    "def df_dx2(x): return (-2) * (68*(2.40 - x[0] - x[1]*68) + 137*(4.70 - x[0] - x[1]*137) + 315*(12 - x[0] - x[1]*315) + \n",
    "405*(14.44 - x[0] - x[1]*405) + 700*(26 - x[0] - x[1]*700))\n",
    "\n",
    "def fd(x): return array([ df_dx1(x), df_dx2(x) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conjugate gradient:\n",
      "iteration 1: \t direction: ['-9660.9200000', '-4612337.4000000'] \t alpha: 0.0000006 \t x: ['2.9937804', '0.0306066']\n",
      "iteration 2: \t direction: ['-10.3292304', '0.0216123'] \t alpha: 0.3125111 \t x: ['-0.2342191', '0.0373607']\n",
      "iteration 3: \t direction: ['0.0000007', '0.0003339'] \t alpha: 0.0000006 \t x: ['-0.2342191', '0.0373607']\n",
      "[-0.23421907  0.03736067]\n"
     ]
    }
   ],
   "source": [
    "x = array([3, 3])\n",
    "precision = 10e-6\n",
    "max_iterations = 5\n",
    "callback = print_all\n",
    "\n",
    "\n",
    "print(\"\\nconjugate gradient:\")\n",
    "value = conjugate_gradient(f, fd, x, max_iterations, precision, callback)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados Númericos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://probability4datascience.com/\n",
    "\n",
    "* http://www.databookuw.com/\n",
    "\n",
    "* Applied Linear Statistical Models\n",
    "\n",
    "* INTRODUCTION TO LINEAR REGRESSION ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c2e15034f07720a3e57266749b805dbe4b964ae6e11e3a0ca2f3b08408e10de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
